#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Apr  9 11:47:19 2018

@author: volvetzhang
"""

import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
import numpy as np



mnist = input_data.read_data_sets('data', one_hot=True)

batch_size = 128
img_size = 28
num_classes = 10
training_epoch = 20

X = tf.placeholder(tf.float32, [None, img_size, img_size, 1])
Y = tf.placeholder(tf.float32, [None, num_classes])

W1 = tf.Variable(tf.truncated_normal([3,3,1,32], stddev=0.1))
W2 = tf.Variable(tf.truncated_normal([3,3,32,64], stddev=0.1))
W3 = tf.Variable(tf.truncated_normal([3,3,64,128], stddev=0.1))
W4 = tf.Variable(tf.truncated_normal([128*4*4, 625], stddev=0.1))
W5 = tf.Variable(tf.truncated_normal([625, num_classes], stddev=0.1))

p_keep_conv = tf.placeholder(tf.float32)
p_keep_hidden = tf.placeholder(tf.float32)

# CNN layer 1
conv1 = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='SAME')
conv1_o = tf.nn.relu(conv1)
conv1 = tf.nn.max_pool(conv1_o, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')
conv1 = tf.nn.dropout(conv1, p_keep_conv)

# CNN layer 2
conv2 = tf.nn.conv2d(conv1, W2, strides=[1,1,1,1], padding='SAME')
conv2_o = tf.nn.relu(conv2)
conv2 = tf.nn.max_pool(conv2_o, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')
conv2 = tf.nn.dropout(conv2, p_keep_conv)

# CNN layer 3
conv3 = tf.nn.conv2d(conv2, W3, strides=[1,1,1,1], padding='SAME')
conv3_o = tf.nn.relu(conv3)
conv3 = tf.nn.max_pool(conv3_o, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')
conv3 = tf.reshape(conv3, [-1, W4.get_shape().as_list()[0]])
conv3 = tf.nn.dropout(conv3, p_keep_conv)

# Hidden layer 4
hidden4 = tf.nn.relu(tf.matmul(conv3, W4))
hidden4 = tf.nn.dropout(hidden4, p_keep_hidden)

# Output layer
output_layer = tf.matmul(hidden4, W5)
#output_layer = model(X, W1, W2, W3, W4, W5, p_keep_conv, p_keep_hidden)

cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=output_layer))
optimizer = tf.train.RMSPropOptimizer(0.001).minimize(cost)
predict_op = tf.argmax(tf.nn.softmax(output_layer), 1)

trX, trY, teX, teY = mnist.train.images,\
                     mnist.train.labels, \
                     mnist.test.images, \
                     mnist.test.labels

trX = trX.reshape(-1, img_size, img_size, 1)  # 28x28x1 input img
teX = teX.reshape(-1, img_size, img_size, 1)  # 28x28x1 input img

with tf.Session() as sess:
    #tf.initialize_all_variables().run()
    tf.global_variables_initializer().run()
    for i in range(training_epoch):
        training_batch = \
                       zip(range(0, len(trX), \
                                 batch_size),
                             range(batch_size, \
                                   len(trX)+1, \
                                   batch_size))
        for start, end in training_batch:
            sess.run(optimizer, feed_dict={X: trX[start:end],\
                                          Y: trY[start:end],\
                                          p_keep_conv: 0.8,\
                                          p_keep_hidden: 0.5})

        print("Epoch: ", i, "Accuracy: ", np.mean(np.argmax(teY, axis=1) ==\
                         sess.run\
                         (predict_op,\
                          feed_dict={X: teX,\
                                     Y: teY, \
                                     p_keep_conv: 1.0,\
                                     p_keep_hidden: 1.0})))


